---
title: "Project 3"
author: "Hendrik Orem, Ph.D., with thanks to Jameson Watts"
date: "03/16/2023"
output:
  pdf_document:
    df_print: kable
    fig_width: 11
    fig_height: 8
  html_document:
    df_print: paged
---





## Setup
```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(caret)
library(fastDummies)
library(randomForest)
# https://www.openml.org/d/1590

raw_income = read_csv("./openml_1590.csv", na=c("?"))

income = read_csv("./openml_1590.csv", na=c("?")) %>%
  drop_na() %>%
  mutate(income_above_50k = class==">50K") %>%
  select(-class) %>%
  dummy_cols(remove_selected_columns = T)

#View(income)
```

## Some questions about the data

Please try at least a few of the following:

* Run PCA on the dataset. How many principal components do you need to explain half the variation? 
* Can you give some interpretation of the principal components?
* Look at the scree plot. How many PCs would you choose based on that?
* Are the first few Principal Components good predictors of income_above_50k?
* How well can you predict income_above_50k using the first 5 or 6 principal components? How about only the first 2?
* Can you gain any insights into the data based on k-means clustering? 
* Can you visualize and interpret some or all of your clusters?
* Using any and all techniques we have learned, build the best predictive model for income_above_50k that you can. What are your best features? What model did you use? What interpretations can you draw?
* What metric can you use to assess model performance? Why is that a good choice of metric in this case?
* What are some key insights you found through your analysis?

Please remember: a statement like "PC2 is not a meaningful predictor for our modeling problem" is a great insight; sometimes things don't work!

10 pts PCA

* analysis and interpretation of factor loadings
* discussion of scree plot and/or analysis of some density plots of PCs
* meaningful interpretation / discussion of conclusions 

10 pts k-means

* discussion for choosing number of clusters
* analysis of cluster centers
* bivariate chart(s) against meaningful variables and/or analysis of density plots
* meaningful interpretation / discussion of conclusions 

##preprocessing before K-means clustering
```{r}
#preprocessing data
income_scaled <- income %>% 
    mutate(age=scale(age), fnlwgt=scale(fnlwgt), `education-num`=scale(`education-num`), `capital-gain`=scale(`capital-gain`), `capital-loss`=scale(`capital-loss`), `hours-per-week`=scale(`hours-per-week`))
```


##Basic K-means clustering
```{r}
#setting # of clusters to 3 (that's K)
kclust <- kmeans(income_scaled, centers = 3)
kclust$centers

```

```{r}
glance(kclust)
```


##Add Clusters to original dataset
```{r}
incomek <- augment(kclust,income_scaled)
head(incomek)
```

##Visualize Clusters
```{r}
#what do these mean?
incomek %>% 
  pivot_longer(c(age, fnlwgt, `education-num`),names_to = "feature") %>% 
  ggplot(aes(value, fill=.cluster))+
  geom_density(alpha=0.3)+
  facet_wrap(~feature)

incomek %>% 
  pivot_longer(c(`capital-gain`,`capital-loss`),names_to = "feature") %>% 
  ggplot(aes(value, fill=.cluster))+
  geom_density(alpha=0.3)+
  facet_wrap(~feature)

incomek %>% 
  pivot_longer(c(`hours-per-week`,workclass_Private),names_to = "feature") %>% 
  ggplot(aes(value, fill=.cluster))+
  geom_density(alpha=0.3)+
  facet_wrap(~feature)

incomek %>% 
  pivot_longer(c(`hours-per-week`,income_above_50k),names_to = "feature") %>% 
  ggplot(aes(value, fill=.cluster))+
  geom_density(alpha=0.3)+
  facet_wrap(~feature)

```

##Try different numbers of clusters
```{r}
kclusts <- tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~kmeans(income_scaled, .x)),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, income_scaled)
  )


```

##Plot the different clusters on two axes
```{r}
assignments <- kclusts %>% 
  unnest(augmented)

#changed from price points from lecture
ggplot(assignments, aes(`hours-per-week`, `education-num`)) +
  geom_point(aes(color = .cluster), alpha=0.3) + 
  facet_wrap(~ k)
```

##Look at improvement in within-cluster error
```{r}
#can still look for elbow (looks like about 7)
clusterings <- kclusts %>%
  unnest(glanced, .drop = TRUE)

ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line()
```


```{r}
# Run k-means with the optimal number of clusters
optimal_k <- 7
kclust_optimal <- kmeans(income_scaled, centers = optimal_k)

# Check the cluster centroids
kclust_optimal$centers

```

```{r}
# Run k-means with 7 clusters
kclust <- kmeans(income_scaled, centers = 7)

# Print the cluster centers
print(kclust$centers)

```

##Outliers? lof
```{r}
library(dbscan)
lof <- lof(income, minPts = 10)
summary(lof)

hist(lof, breaks = 10, main = "LOF (minPts = 10)")

plot(sort(lof), type = "l",  main = "LOF (minPts = 10)",
  xlab = "Points sorted by LOF", ylab = "LOF")

#capital gain/loss
plot(select(income, c("capital-gain", "capital-loss")), pch = ".", main = "LOF (minPts = 10)", asp = 1)
points(select(income, c("capital-gain", "capital-loss")), cex = (lof - 1) * 4, pch = 1, col = "red")
text(income[lof > 1.3,], labels = round(lof, 1)[lof > 1.3], pos = 3)

#education age
plot(select(income, c("education-num", "age")), pch = ".", main = "LOF (minPts = 10)", asp = 1)
points(select(income, c("education-num", "age")), cex = (lof - 1) * 4, pch = 1, col = "red")
text(income[lof > 1.3,], labels = round(lof, 1)[lof > 1.3], pos = 3)


```

##Outliers isolation forest
```{r}
library(isotree)
model = isolation.forest(income, ndim=1, ntrees=10)
scores = predict(model, income, type="score")
hist(scores, breaks = 10, main = "IF Scores")

plot(sort(scores), type = "l",  main = "IF Scores",
  xlab = "Points sorted by score", ylab = "IF score")

plot(select(income, c(`hours-per-week`, `education-num`)), pch = ".", main = "IF Scores", asp = 1)
points(select(income, c(`hours-per-week`, `education-num`))[scores > 0.5,], cex = as.data.frame(scores)[scores > 0.5,])
       
```

##Model kappa .57 with no feature engineering
```{r}
#income$income_above_50k <- as.factor(income$income_above_50k)
income$income_above_50k <- factor(income$income_above_50k, levels = c(FALSE, TRUE), labels = c("Below_50K", "Above_50K"))


#timer
start_time <- Sys.time()


# specify the model to be used (i.e. KNN, Naive Bayes, decision tree, random forest, bagged trees) and the tuning parameters used
ctrl <- trainControl(method = "cv", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)
set.seed(504) 

income_index <- createDataPartition(income$income_above_50k, p = 0.80, list = FALSE)
train <- income[ income_index, ]
test <- income[-income_index, ]

# example spec for rf
fit <- train(income_above_50k ~ .,
             data = train, 
             method = "rf",
             ntree = 20, 
             tuneLength = 3,
             metric = "ROC",
             trControl = ctrl)

fit

confusionMatrix(predict(fit, test),factor(test$income_above_50k))


#end timer
end_time <- Sys.time()
time_taken <- end_time - start_time
print(time_taken)
print(as.numeric(time_taken, units = "mins"))
```


10 pts supervised learning

* feature engineering / selection, whether with PCA or otherwise
* interpretation of variable importance, coefficients if applicable
* justification of choice of metric
* discussion of choice or tuning of hyperparameters, if any
* meaningful discussion of predictive power and conclusions from model

Please be prepared to 

* Submit your Rmd + compiled html or pdf, *and*
* Present your findings to the class in a compelling way, speaking for 10 minutes or so. You don't need to cover everything in your analysis that you submit to me, focus on the fun / interesting / compelling highlights or challenges.



##EDA
```{r}

# Replace 'Income' with the actual column name in your dataset
income$income_above_50k <- as.numeric(factor(income$income_above_50k, levels = unique(income$income_above_50k))) - 1

# Display basic information about the dataset
str(income)

# Set the figure size for the plots
library(ggplot2)
options(repr.plot.width = 20, repr.plot.height = 12)

# Histograms for selected columns
hist_cols <- c("age", "fnlwgt", "education-num", "capital-gain", "capital-loss", "hours-per-week")
par(mfrow=c(2, 3))
for (col in hist_cols) {
  hist(income[[col]], main=col, xlab=col, col="lightblue")
}

```


####Correlation Matrix Heatmap
```{r}
library(ggcorrplot)

# Select the columns you want to include in the correlation matrix
selected_columns <- c("age", "fnlwgt", "education-num", "capital-gain", "capital-loss", "hours-per-week", "income_above_50k")

# Calculate the correlation matrix for the selected columns
corr_matrix <- cor(income[, selected_columns])

# Create a heatmap of the correlation matrix with correlation coefficients
ggcorrplot(corr_matrix, lab = TRUE, lab_size = 4, tl.cex = 12, tl.col = "black", tl.srt = 45)


```

##Bucket Age
```{r}
# Load the required packages
library(ggplot2)
library(dplyr)

# Bin the 'age' column
income <- income %>%
  mutate(age_group = case_when(
    age >= 0 & age <= 25 ~ "young",
    age > 25 & age <= 65 ~ "prime",
    age > 65 & age <= 100 ~ "retired"
  ))

# Create the count plot with 'income_above_50k' as the hue
ggplot(income, aes(x = age_group, fill = as.factor(income_above_50k))) +
  geom_bar(position = "dodge") +
  labs(x = "Age Group", y = "Count", fill = "Income Above 50K") +
  theme_minimal()

```

##
```{r}
library(dplyr)
library(ggplot2)

# Create 'Capital Diff' column and remove 'Capital Gain' and 'Capital Loss' columns
income <- income %>%
  mutate(capital_diff = `capital-gain` - `capital-loss`) %>%
  select(-`capital-gain`, -`capital-loss`)

# Bin the 'Capital Diff' column
income <- income %>%
  mutate(capital_diff = case_when(
    capital_diff >= -5000 & capital_diff <= 5000 ~ "Minor",
    capital_diff > 5000 & capital_diff <= 100000 ~ "Major"
  ))

# Create the count plot with 'Income' as the hue
ggplot(income, aes(x = capital_diff, fill = income_above_50k)) +
  geom_bar(position = "dodge") +
  labs(x = "Capital Diff", y = "Count", fill = "Income") +
  theme_minimal()

#had error so ran this then ran it again
#check if factor
class(income$income_above_50k)

# Convert 'income_above_50k' to a factor
income$income_above_50k <- as.factor(income$income_above_50k)
```

##Drop Columns
```{r}
income <- income %>% select(-fnlwgt)

income <- income %>% select(-`workclass_Without-pay`)
```

##Bin Hours
```{r}
# Bin the 'Hours per Week' column
income <- income %>%
  mutate(hours_per_week = case_when(
    `hours-per-week` >= 0 & `hours-per-week` <= 32 ~ "Part Time",
    `hours-per-week` > 32 & `hours-per-week` <= 40 ~ "Full Time",
    `hours-per-week` > 40 & `hours-per-week` <= 100 ~ "Overtime"
  ))

# Create the count plot with 'Income' as the hue
ggplot(income, aes(x = hours_per_week, fill = income_above_50k)) +
  geom_bar(position = "dodge") +
  labs(x = "Hours per Week", y = "Count", fill = "Income") +
  theme_minimal()

```
##Looking at Education
```{r}

ggplot(raw_income, aes(x = education, fill = class)) +
  geom_bar(position = "dodge") +
  labs(x = "Education", y = "Count", fill = "Income") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
##
```{r}

```

